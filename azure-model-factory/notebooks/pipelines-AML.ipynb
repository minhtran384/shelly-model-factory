{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install required libraries\n",
        "\n",
        "Install the libraries only once. It is recommended restarting the notebook kernel after libraries being installed."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U azureml-sdk==1.26.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azureml-sdk==1.26.0\n",
            "  Downloading azureml_sdk-1.26.0-py3-none-any.whl (4.4 kB)\n",
            "Collecting azureml-dataset-runtime[fuse]~=1.26.0\n",
            "  Downloading azureml_dataset_runtime-1.26.0-py3-none-any.whl (3.4 kB)\n",
            "Collecting azureml-train~=1.26.0\n",
            "  Downloading azureml_train-1.26.0-py3-none-any.whl (3.3 kB)\n",
            "Collecting azureml-core~=1.26.0\n",
            "  Downloading azureml_core-1.26.0.post1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 13.1 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting azureml-train-automl-client~=1.26.0\n",
            "  Downloading azureml_train_automl_client-1.26.0-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 63.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting azureml-pipeline~=1.26.0\n",
            "  Downloading azureml_pipeline-1.26.0-py3-none-any.whl (3.7 kB)\n",
            "Collecting pyarrow<2.0.0,>=0.17.0\n",
            "  Downloading pyarrow-1.0.1-cp36-cp36m-manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 56.0 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy!=1.19.3; sys_platform == \"linux\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataset-runtime[fuse]~=1.26.0->azureml-sdk==1.26.0) (1.18.5)\n",
            "Collecting azureml-dataprep<2.14.0a,>=2.13.0a\n",
            "  Downloading azureml_dataprep-2.13.2-py3-none-any.whl (39.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 39.4 MB 2.5 kB/s  eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: fusepy<4.0.0,>=3.0.1; extra == \"fuse\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataset-runtime[fuse]~=1.26.0->azureml-sdk==1.26.0) (3.0.1)\n",
            "Collecting azureml-train-core~=1.26.0\n",
            "  Downloading azureml_train_core-1.26.0-py3-none-any.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 20.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: azure-mgmt-resource<15.0.0,>=1.2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (13.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (2021.1)\n",
            "Requirement already satisfied, skipping upgrade: pathspec in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: azure-mgmt-authorization<1.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.61.0)\n",
            "Requirement already satisfied, skipping upgrade: ndg-httpsclient in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: azure-graphrbac<1.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.61.1)\n",
            "Requirement already satisfied, skipping upgrade: pyopenssl<21.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (20.0.1)\n",
            "Requirement already satisfied, skipping upgrade: jmespath in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: docker in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (4.4.4)\n",
            "Requirement already satisfied, skipping upgrade: PyJWT<3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.19.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (2.25.1)\n",
            "Requirement already satisfied, skipping upgrade: msrest>=0.5.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.6.21)\n",
            "Requirement already satisfied, skipping upgrade: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (3.4.7)\n",
            "Requirement already satisfied, skipping upgrade: ruamel.yaml<0.17.5,>=0.15.35 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.17.4)\n",
            "Requirement already satisfied, skipping upgrade: azure-mgmt-containerregistry>=2.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (8.0.0)\n",
            "Requirement already satisfied, skipping upgrade: contextlib2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.6.0.post1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.23 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (1.25.11)\n",
            "Collecting azure-mgmt-keyvault<7.0.0,>=0.40.0\n",
            "  Downloading azure_mgmt_keyvault-2.2.0-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 8.8 MB/s  eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: adal>=1.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (1.2.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: msrestazure>=0.4.33 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.6.4)\n",
            "Requirement already satisfied, skipping upgrade: azure-mgmt-storage<16.0.0,>=1.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (11.2.0)\n",
            "Requirement already satisfied, skipping upgrade: SecretStorage in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (3.3.1)\n",
            "Requirement already satisfied, skipping upgrade: backports.tempfile in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (1.0)\n",
            "Requirement already satisfied, skipping upgrade: azure-common>=1.1.12 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (1.1.27)\n",
            "Requirement already satisfied, skipping upgrade: jsonpickle in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.26.0->azureml-sdk==1.26.0) (2.0.0)\n",
            "Collecting azureml-telemetry~=1.26.0\n",
            "  Downloading azureml_telemetry-1.26.0-py3-none-any.whl (30 kB)\n",
            "Collecting azureml-automl-core~=1.26.0\n",
            "  Downloading azureml_automl_core-1.26.0-py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 61.1 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting azureml-pipeline-core~=1.26.0\n",
            "  Downloading azureml_pipeline_core-1.26.0-py3-none-any.whl (309 kB)\n",
            "\u001b[K     |████████████████████████████████| 309 kB 48.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting azureml-pipeline-steps~=1.26.0\n",
            "  Downloading azureml_pipeline_steps-1.26.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 4.5 MB/s  eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: dotnetcore2<3.0.0,>=2.1.14 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataprep<2.14.0a,>=2.13.0a->azureml-dataset-runtime[fuse]~=1.26.0->azureml-sdk==1.26.0) (2.1.21)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle<2.0.0,>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataprep<2.14.0a,>=2.13.0a->azureml-dataset-runtime[fuse]~=1.26.0->azureml-sdk==1.26.0) (1.6.0)\n",
            "Collecting azureml-dataprep-native<33.0.0,>=32.0.0\n",
            "  Downloading azureml_dataprep_native-32.0.0-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting azureml-dataprep-rslex<1.12.0a,>=1.11.0dev0\n",
            "  Downloading azureml_dataprep_rslex-1.11.2-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 30.0 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: azure-identity<1.5.0,>=1.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-dataprep<2.14.0a,>=2.13.0a->azureml-dataset-runtime[fuse]~=1.26.0->azureml-sdk==1.26.0) (1.4.1)\n",
            "Collecting azureml-train-restclients-hyperdrive~=1.26.0\n",
            "  Downloading azureml_train_restclients_hyperdrive-1.26.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ndg-httpsclient->azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from pyopenssl<21.0.0->azureml-core~=1.26.0->azureml-sdk==1.26.0) (1.16.0)\n",
            "Requirement already satisfied, skipping upgrade: websocket-client>=0.32.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from docker->azureml-core~=1.26.0->azureml-sdk==1.26.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<5,>=3.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3.0.0,>=2.19.1->azureml-core~=1.26.0->azureml-sdk==1.26.0) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3.0.0,>=2.19.1->azureml-core~=1.26.0->azureml-sdk==1.26.0) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3.0.0,>=2.19.1->azureml-core~=1.26.0->azureml-sdk==1.26.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest>=0.5.1->azureml-core~=1.26.0->azureml-sdk==1.26.0) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: isodate>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest>=0.5.1->azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: cffi>=1.12 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0->azureml-core~=1.26.0->azureml-sdk==1.26.0) (1.14.5)\n",
            "Requirement already satisfied, skipping upgrade: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.10\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ruamel.yaml<0.17.5,>=0.15.35->azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: azure-mgmt-core<2.0.0,>=1.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-mgmt-containerregistry>=2.0.0->azureml-core~=1.26.0->azureml-sdk==1.26.0) (1.2.2)\n",
            "Requirement already satisfied, skipping upgrade: jeepney>=0.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from SecretStorage->azureml-core~=1.26.0->azureml-sdk==1.26.0) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: backports.weakref in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from backports.tempfile->azureml-core~=1.26.0->azureml-sdk==1.26.0) (1.0.post1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from jsonpickle->azureml-core~=1.26.0->azureml-sdk==1.26.0) (4.5.0)\n",
            "Requirement already satisfied, skipping upgrade: applicationinsights in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-telemetry~=1.26.0->azureml-train-automl-client~=1.26.0->azureml-sdk==1.26.0) (0.11.10)\n",
            "Requirement already satisfied, skipping upgrade: distro>=1.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from dotnetcore2<3.0.0,>=2.1.14->azureml-dataprep<2.14.0a,>=2.13.0a->azureml-dataset-runtime[fuse]~=1.26.0->azureml-sdk==1.26.0) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: azure-core<2.0.0,>=1.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-identity<1.5.0,>=1.2.0->azureml-dataprep<2.14.0a,>=2.13.0a->azureml-dataset-runtime[fuse]~=1.26.0->azureml-sdk==1.26.0) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: msal-extensions~=0.2.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-identity<1.5.0,>=1.2.0->azureml-dataprep<2.14.0a,>=2.13.0a->azureml-dataset-runtime[fuse]~=1.26.0->azureml-sdk==1.26.0) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: msal<2.0.0,>=1.3.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure-identity<1.5.0,>=1.2.0->azureml-dataprep<2.14.0a,>=2.13.0a->azureml-dataset-runtime[fuse]~=1.26.0->azureml-sdk==1.26.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.1->azureml-core~=1.26.0->azureml-sdk==1.26.0) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cffi>=1.12->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0->azureml-core~=1.26.0->azureml-sdk==1.26.0) (2.20)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->azureml-core~=1.26.0->azureml-sdk==1.26.0) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->azureml-core~=1.26.0->azureml-sdk==1.26.0) (3.10.0.0)\n",
            "Requirement already satisfied, skipping upgrade: portalocker~=1.0; platform_system != \"Windows\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msal-extensions~=0.2.2->azure-identity<1.5.0,>=1.2.0->azureml-dataprep<2.14.0a,>=2.13.0a->azureml-dataset-runtime[fuse]~=1.26.0->azureml-sdk==1.26.0) (1.7.1)\n",
            "\u001b[31mERROR: azureml-widgets 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-widgets 1.31.0 has requirement azureml-telemetry~=1.31.0, but you'll have azureml-telemetry 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-train-automl 1.31.0 has requirement azureml-automl-core~=1.31.0, but you'll have azureml-automl-core 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-train-automl 1.31.0 has requirement azureml-dataset-runtime[fuse,pandas]~=1.31.0, but you'll have azureml-dataset-runtime 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-train-automl 1.31.0 has requirement azureml-train-automl-client~=1.31.0, but you'll have azureml-train-automl-client 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-train-automl-runtime 1.31.0 has requirement azureml-automl-core~=1.31.0, but you'll have azureml-automl-core 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-train-automl-runtime 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-train-automl-runtime 1.31.0 has requirement azureml-dataset-runtime[fuse,pandas]~=1.31.0, but you'll have azureml-dataset-runtime 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-train-automl-runtime 1.31.0 has requirement azureml-telemetry~=1.31.0, but you'll have azureml-telemetry 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-train-automl-runtime 1.31.0 has requirement azureml-train-automl-client~=1.31.0, but you'll have azureml-train-automl-client 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-tensorboard 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-responsibleai 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-responsibleai 1.31.0 has requirement azureml-dataset-runtime~=1.31.0, but you'll have azureml-dataset-runtime 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-opendatasets 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-opendatasets 1.31.0 has requirement azureml-dataset-runtime[fuse,pandas]~=1.31.0, but you'll have azureml-dataset-runtime 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-opendatasets 1.31.0 has requirement azureml-telemetry~=1.31.0, but you'll have azureml-telemetry 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-opendatasets 1.31.0 has requirement scipy<=1.4.1,>=1.0.0, but you'll have scipy 1.5.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-mlflow 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-interpret 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-defaults 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-defaults 1.31.0 has requirement azureml-dataset-runtime[fuse]~=1.31.0, but you'll have azureml-dataset-runtime 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-datadrift 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-datadrift 1.31.0 has requirement azureml-dataset-runtime[fuse,pandas]~=1.31.0, but you'll have azureml-dataset-runtime 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-datadrift 1.31.0 has requirement azureml-pipeline-core~=1.31.0, but you'll have azureml-pipeline-core 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-datadrift 1.31.0 has requirement azureml-telemetry~=1.31.0, but you'll have azureml-telemetry 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-services 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-server 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-reinforcementlearning 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-reinforcementlearning 1.31.0 has requirement azureml-train-core~=1.31.0, but you'll have azureml-train-core 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-pipeline-steps 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-pipeline-steps 1.31.0 has requirement azureml-dataset-runtime~=1.31.0, but you'll have azureml-dataset-runtime 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-pipeline-steps 1.31.0 has requirement azureml-pipeline-core~=1.31.0, but you'll have azureml-pipeline-core 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-notebook 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-notebook 1.31.0 has requirement azureml-pipeline-core~=1.31.0, but you'll have azureml-pipeline-core 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-fairness 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-dataset 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-contrib-dataset 1.31.0 has requirement azureml-dataset-runtime[fuse,pandas]~=1.31.0, but you'll have azureml-dataset-runtime 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-cli-common 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-cli-common 1.31.0 has requirement azureml-pipeline-core~=1.31.0, but you'll have azureml-pipeline-core 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-cli-common 1.31.0 has requirement azureml-train-core~=1.31.0; python_version >= \"3.5\", but you'll have azureml-train-core 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-automl-runtime 1.31.0 has requirement azureml-automl-core~=1.31.0, but you'll have azureml-automl-core 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-automl-runtime 1.31.0 has requirement azureml-dataset-runtime[fuse,pandas]~=1.31.0, but you'll have azureml-dataset-runtime 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-automl-dnn-nlp 1.31.0 has requirement azureml-automl-core~=1.31.0, but you'll have azureml-automl-core 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-automl-dnn-nlp 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-automl-dnn-nlp 1.31.0 has requirement azureml-telemetry~=1.31.0, but you'll have azureml-telemetry 1.26.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azureml-accel-models 1.31.0 has requirement azureml-core~=1.31.0, but you'll have azureml-core 1.26.0.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azure-cli 2.25.0 has requirement azure-graphrbac~=0.60.0, but you'll have azure-graphrbac 0.61.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azure-cli 2.25.0 has requirement azure-mgmt-containerregistry==3.0.0rc17, but you'll have azure-mgmt-containerregistry 8.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azure-cli 2.25.0 has requirement azure-mgmt-keyvault==9.0.0, but you'll have azure-mgmt-keyvault 2.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azure-cli 2.25.0 has requirement azure-mgmt-resource==18.0.0, but you'll have azure-mgmt-resource 13.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azure-cli 2.25.0 has requirement azure-mgmt-storage~=18.0.0, but you'll have azure-mgmt-storage 11.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azure-cli 2.25.0 has requirement pytz==2019.1, but you'll have pytz 2021.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: azure-cli 2.25.0 has requirement websocket-client~=0.56.0, but you'll have websocket-client 1.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyarrow, azureml-dataprep-native, azureml-dataprep-rslex, azureml-dataprep, azureml-dataset-runtime, azure-mgmt-keyvault, azureml-core, azureml-telemetry, azureml-train-restclients-hyperdrive, azureml-train-core, azureml-train, azureml-automl-core, azureml-train-automl-client, azureml-pipeline-core, azureml-pipeline-steps, azureml-pipeline, azureml-sdk\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Attempting uninstall: azureml-dataprep-native\n",
            "    Found existing installation: azureml-dataprep-native 36.0.0\n",
            "    Uninstalling azureml-dataprep-native-36.0.0:\n",
            "      Successfully uninstalled azureml-dataprep-native-36.0.0\n",
            "  Attempting uninstall: azureml-dataprep-rslex\n",
            "    Found existing installation: azureml-dataprep-rslex 1.16.1\n",
            "    Uninstalling azureml-dataprep-rslex-1.16.1:\n",
            "      Successfully uninstalled azureml-dataprep-rslex-1.16.1\n",
            "  Attempting uninstall: azureml-dataprep\n",
            "    Found existing installation: azureml-dataprep 2.18.0\n",
            "    Uninstalling azureml-dataprep-2.18.0:\n",
            "      Successfully uninstalled azureml-dataprep-2.18.0\n",
            "  Attempting uninstall: azureml-dataset-runtime\n",
            "    Found existing installation: azureml-dataset-runtime 1.31.0\n",
            "    Uninstalling azureml-dataset-runtime-1.31.0:\n",
            "      Successfully uninstalled azureml-dataset-runtime-1.31.0\n",
            "  Attempting uninstall: azure-mgmt-keyvault\n",
            "    Found existing installation: azure-mgmt-keyvault 9.0.0\n",
            "    Uninstalling azure-mgmt-keyvault-9.0.0:\n",
            "      Successfully uninstalled azure-mgmt-keyvault-9.0.0\n",
            "  Attempting uninstall: azureml-core\n",
            "    Found existing installation: azureml-core 1.31.0\n",
            "    Uninstalling azureml-core-1.31.0:\n",
            "      Successfully uninstalled azureml-core-1.31.0\n",
            "  Attempting uninstall: azureml-telemetry\n",
            "    Found existing installation: azureml-telemetry 1.31.0\n",
            "    Uninstalling azureml-telemetry-1.31.0:\n",
            "      Successfully uninstalled azureml-telemetry-1.31.0\n",
            "  Attempting uninstall: azureml-train-restclients-hyperdrive\n",
            "    Found existing installation: azureml-train-restclients-hyperdrive 1.31.0\n",
            "    Uninstalling azureml-train-restclients-hyperdrive-1.31.0:\n",
            "      Successfully uninstalled azureml-train-restclients-hyperdrive-1.31.0\n",
            "  Attempting uninstall: azureml-train-core\n",
            "    Found existing installation: azureml-train-core 1.31.0\n",
            "    Uninstalling azureml-train-core-1.31.0:\n",
            "      Successfully uninstalled azureml-train-core-1.31.0\n",
            "  Attempting uninstall: azureml-train\n",
            "    Found existing installation: azureml-train 1.31.0\n",
            "    Uninstalling azureml-train-1.31.0:\n",
            "      Successfully uninstalled azureml-train-1.31.0\n",
            "  Attempting uninstall: azureml-automl-core\n",
            "    Found existing installation: azureml-automl-core 1.31.0\n",
            "    Uninstalling azureml-automl-core-1.31.0:\n",
            "      Successfully uninstalled azureml-automl-core-1.31.0\n",
            "  Attempting uninstall: azureml-train-automl-client\n",
            "    Found existing installation: azureml-train-automl-client 1.31.0\n",
            "    Uninstalling azureml-train-automl-client-1.31.0:\n",
            "      Successfully uninstalled azureml-train-automl-client-1.31.0\n",
            "  Attempting uninstall: azureml-pipeline-core\n",
            "    Found existing installation: azureml-pipeline-core 1.31.0\n",
            "    Uninstalling azureml-pipeline-core-1.31.0:\n",
            "      Successfully uninstalled azureml-pipeline-core-1.31.0\n",
            "  Attempting uninstall: azureml-pipeline-steps\n",
            "    Found existing installation: azureml-pipeline-steps 1.31.0\n",
            "    Uninstalling azureml-pipeline-steps-1.31.0:\n",
            "      Successfully uninstalled azureml-pipeline-steps-1.31.0\n",
            "  Attempting uninstall: azureml-pipeline\n",
            "    Found existing installation: azureml-pipeline 1.31.0\n",
            "    Uninstalling azureml-pipeline-1.31.0:\n",
            "      Successfully uninstalled azureml-pipeline-1.31.0\n",
            "  Attempting uninstall: azureml-sdk\n",
            "    Found existing installation: azureml-sdk 1.31.0\n",
            "    Uninstalling azureml-sdk-1.31.0:\n",
            "      Successfully uninstalled azureml-sdk-1.31.0\n",
            "Successfully installed azure-mgmt-keyvault-2.2.0 azureml-automl-core-1.26.0 azureml-core-1.26.0.post1 azureml-dataprep-2.13.2 azureml-dataprep-native-32.0.0 azureml-dataprep-rslex-1.11.2 azureml-dataset-runtime-1.26.0 azureml-pipeline-1.26.0 azureml-pipeline-core-1.26.0 azureml-pipeline-steps-1.26.0 azureml-sdk-1.26.0 azureml-telemetry-1.26.0 azureml-train-1.26.0 azureml-train-automl-client-1.26.0 azureml-train-core-1.26.0 azureml-train-restclients-hyperdrive-1.26.0 pyarrow-1.0.1\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Azure Machine Learning Pipelines\n",
        "\n",
        "This notebook shows how Contoso Auto can benefit from creating re-usable Azure machine learning pipelines.\n",
        "\n",
        "The goal is to build a pipeline that demonstrates the basic data science workflow of data preparation, model training, and predictions. With Azure Machine Learning, distinct steps are defined and it is possible to re-use these pipeline steps as well as to rerun only the steps when needed as you tweak and test your workflow.\n",
        "\n",
        "A subset of data is collected from Contoso Auto's fleet management program. The data is enriched with holiday and weather data. The goal is to train a regression model to predict taxi fares in New York City based on input features such as, number of passengers, trip distance, datetime, holiday information and weather information.\n",
        "\n",
        "The machine learning pipeline in this notebook is organized into three steps:\n",
        "\n",
        "- **Preprocess Training and Input Data:** Preprocess the data to better represent the datetime features, such as hours of the day, and day of the week to capture the cyclical nature of these features.\n",
        "\n",
        "- **Model Training:** Use GradientBoostingRegressor algorithm scikit-learn library to train a regression model. The trained model will be saved for later making predictions.\n",
        "\n",
        "- **Model Inference:**  Bulk predictions. Each time an input file is uploaded to a data store, bulk model predictions is initiated on the input data. Before model predictions, the preprocess data step is re-used to process input data.\n",
        "\n",
        "Each of these pipelines is going to have implicit data dependencies and AML make it possible to re-use previously completed steps and run only the modified or new steps in your pipeline.\n",
        "\n",
        "The pipelines will be run on the Azure Machine Learning compute."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Azure Machine Learning and Pipeline SDK-specific Imports"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Experiment, Datastore\n",
        "from azureml.data.azure_storage_datastore import AzureBlobDatastore\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)\n",
        "\n",
        "from azureml.data.data_reference import DataReference\n",
        "from azureml.pipeline.core import Pipeline, PipelineData, PipelineRun, StepRun\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "print(\"Pipeline SDK-specific imports completed\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK version: 1.31.0\n",
            "Pipeline SDK-specific imports completed\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1626279656729
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "Set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n",
        "\n",
        "To get these values, do the following:\n",
        "1.  Navigate to the Azure Portal and login with the credentials provided.\n",
        "2.  From the left hand menu, under Favorites, select `Resource Groups`.\n",
        "3.  In the list, select the resource group with the name similar to `XXXXX`.\n",
        "4.  From the Overview tab, capture the desired values.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Provide the Subscription ID of your existing Azure subscription\n",
        "subscription_id = \"8c12de75-5827-4795-b948-6c427fc4b7ab\" # <- needs to be the subscription with the Quick-Starts resource group\n",
        "\n",
        "#Provide values for the existing Resource Group \n",
        "resource_group = \"Shelly.Xiao.RG\" # <- replace XXXXX with your unique identifier\n",
        "\n",
        "#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\n",
        "workspace_name = \"shelly-demo\" # <- replace XXXXX with your unique identifier (should be lowercase)\n",
        "workspace_region = \"westus2\" # <- region of your Quick-Starts resource group\n",
        "\n",
        "aml_compute_target = \"gpucluster\" # <- the name of your GPU-enabled cluster\n",
        "\n",
        "experiment_name = 'fleet-pipeline'\n",
        "\n",
        "# project folder for the script files\n",
        "project_folder = 'aml-pipelines-scripts'\n",
        "data_location = 'aml-pipelines-data'\n",
        "test_data_location = 'aml-pipelines-test-data'"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1626279661539
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_reference_name = 'nyc_taxi_raw_features'\n",
        "feature_data = 'nyc-taxi-raw-features/nyc-taxi-sample-data.csv'"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626279665219
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and connect to an Azure Machine Learning Workspace\n",
        "\n",
        "Run the following cell to access your Azure Machine Learning **Workspace**.\n",
        "\n",
        "**Important Note**: You will be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# By using the exist_ok param, if the worskpace already exists you get a reference to the existing workspace\n",
        "# allowing you to re-run this cell multiple times as desired (which is fairly common in notebooks).\n",
        "ws = Workspace.create(\n",
        "    name = workspace_name,\n",
        "    subscription_id = subscription_id,\n",
        "    resource_group = resource_group, \n",
        "    location = workspace_region,\n",
        "    exist_ok = True)\n",
        "\n",
        "ws.write_config()\n",
        "print('Workspace configuration succeeded')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Workspace configuration succeeded\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1626279671898
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create AML Compute Cluster\n",
        "\n",
        "Azure Machine Learning Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Let's create a new Aml Compute in the current workspace, if it doesn't already exist. We will run all our pipelines on this compute target."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "try:\n",
        "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
        "    print(\"found existing compute target.\")\n",
        "except ComputeTargetException:\n",
        "    print(\"creating new compute target\")\n",
        "    \n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC12\",\n",
        "                                                                min_nodes = 1, \n",
        "                                                                max_nodes = 1)    \n",
        "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
        "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "    \n",
        "print(\"Aml Compute attached\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating new compute target\n",
            "Creating......\n",
            "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
            "Succeeded...........................\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n",
            "Aml Compute attached\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1626279852714
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Run Configuration"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.runconfig import RunConfiguration, DockerConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE,  DEFAULT_GPU_IMAGE\n",
        "\n",
        "# Create a new runconfig object\n",
        "run_amlcompute = RunConfiguration()\n",
        "\n",
        "# Use the cluster you created above. \n",
        "run_amlcompute.target = aml_compute_target\n",
        "\n",
        "# Enable Docker\n",
        "#docker_config = DockerConfiguration(use_docker=True)\n",
        "run_amlcompute.environment.docker.enabled = True\n",
        "\n",
        "# Set Docker base image to the default CPU/GPU-based image\n",
        "run_amlcompute.environment.docker.base_image = DEFAULT_GPU_IMAGE\n",
        "\n",
        "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
        "run_amlcompute.environment.python.user_managed_dependencies = False\n",
        "\n",
        "# Specify CondaDependencies obj, add necessary packages\n",
        "run_amlcompute.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=[\n",
        "    'numpy',\n",
        "    'pandas',\n",
        "    'joblib',\n",
        "    'scikit-learn',\n",
        "    'sklearn-pandas==2.0.0'\n",
        "])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1626281702729
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline Step 1 - Process Training Data "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process training data step in the pipeline takes raw training data as input. This data can be a data source that lives in one of the accessible data locations, or intermediate data produced by a previous step in the pipeline. In this example we will upload the raw training data in the workspace's default blob store. Run the following two cells at the end of which we will create a **DataReference** object that points to the raw training data *csv* file stored in the default blob store."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Default datastore (Azure file storage)\n",
        "def_file_store = ws.get_default_datastore() \n",
        "print(\"Default datastore's name: {}\".format(def_file_store.name))\n",
        "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
        "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default datastore's name: workspaceblobstore\n",
            "Blobstore's name: workspaceblobstore\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1626281713717
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_data = DataReference(datastore=def_blob_store, \n",
        "                                      data_reference_name=data_reference_name, \n",
        "                                      path_on_datastore=feature_data)\n",
        "print(\"DataReference object created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataReference object created\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1626281720857
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Process Training Data Pipeline Step"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within a pipeline, any intermediate data (e.g., the output of a previous Step) is represented by a PipelineData object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps.\n",
        "\n",
        "In the following cell, an instance of the PythonScriptStep is created. It takes a reference to input data, a script to run against the input data and a destination to write the output data that results.\n",
        "\n",
        "Specifically, the step takes the `raw_train_data` DataReference object as input, and it will output an intermediate PipelineData object represented by `processed_train_data` that holds the processed training data. \n",
        "\n",
        "This output data will have new engineered features for the datetime components: hour of the day, and day of the week, as scripted in `preprocess.py`. \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "processed_train_data = PipelineData('processed_train_data', datastore=def_blob_store)\n",
        "print(\"PipelineData object created\")\n",
        "\n",
        "processTrainDataStep = PythonScriptStep(\n",
        "    name=\"process_train_data\",\n",
        "    script_name=\"preprocess.py\", \n",
        "    arguments=[\"--process_mode\", 'train',\n",
        "               \"--input\", raw_train_data,\n",
        "               \"--output\", processed_train_data],\n",
        "    inputs=[raw_train_data],\n",
        "    outputs=[processed_train_data],\n",
        "    compute_target=aml_compute,\n",
        "    runconfig=run_amlcompute,\n",
        "    source_directory=project_folder\n",
        ")\n",
        "print(\"preprocessStep created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PipelineData object created\n",
            "preprocessStep created\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1626281725689
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline Step 2 -  Train Pipeline Step"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train pipeline step takes the `processed_train_data` created in the above step as input and generates another PipelineData object to save the model that results as its output in `trained_model`. This is an example of how machine learning pipelines can have many steps and these steps can use (or reuse) datasources and intermediate data.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = PipelineData('trained_model', datastore=def_blob_store)\n",
        "print(\"PipelineData object created\")\n",
        "\n",
        "trainStep = PythonScriptStep(\n",
        "    name=\"train\",\n",
        "    script_name=\"train.py\", \n",
        "    arguments=[\"--input\", processed_train_data, \"--output\", trained_model],\n",
        "    inputs=[processed_train_data],\n",
        "    outputs=[trained_model],\n",
        "    compute_target=aml_compute,\n",
        "    runconfig=run_amlcompute,\n",
        "    source_directory=project_folder\n",
        ")\n",
        "print(\"trainStep created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PipelineData object created\n",
            "trainStep created\n"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1626281730749
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and Validate the Pipeline\n",
        "Now the core steps have been defined, it is time to define the actual Pipeline object.\n",
        "\n",
        "The `trainStep` has an implicit data dependency on the `processTrainDataStep`. As such, you need only include the `trainStep` in your Pipeline object. \n",
        "\n",
        "You will observe that when you run the pipeline that it will first run the **processTrainDataStep** followed by the **trainStep**.\n",
        "\n",
        "Run the following cell to create the Pipeline and validate it is configured correctly (e.g., check for issues like disconnected inputs)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(workspace=ws, steps=[trainStep])\n",
        "print (\"Pipeline is built\")\n",
        "\n",
        "pipeline.validate()\n",
        "print(\"Simple validation complete\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline is built\n",
            "Step train is ready to be created [609f90f6]Step process_train_data is ready to be created [3c47be96]\n",
            "\n",
            "Simple validation complete\n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1626281739830
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit the Pipeline\n",
        "\n",
        "Next, submit the pipeline for execution."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_run = Experiment(ws, experiment_name).submit(pipeline)\n",
        "print(\"Pipeline is submitted for execution\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created step train [609f90f6][e63e9316-fba3-40e1-abef-a2fbeebabc50], (This step will run and generate new outputs)\n",
            "Created step process_train_data [3c47be96][873bdeb9-f288-46a7-b952-29d9e42dfbb0], (This step will run and generate new outputs)\n",
            "Using data reference nyc_taxi_raw_features for StepId [2cad3ef2][1471eda0-0460-46e8-9d99-b6bba7aee8b2], (Consumers of this data are eligible to reuse prior runs.)\n",
            "Submitted PipelineRun 1378a21d-6b63-48c9-9ad4-65f187f1e210\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/1378a21d-6b63-48c9-9ad4-65f187f1e210?wsid=/subscriptions/8c12de75-5827-4795-b948-6c427fc4b7ab/resourcegroups/Shelly.Xiao.RG/workspaces/shelly-demo&tid=570057f4-73ef-41c8-bcbb-08db2fc15c2b\n",
            "Pipeline is submitted for execution\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1626281750302
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitor the Run Details\n",
        "\n",
        "The pipeline run will take **about 8 minutes to complete.**\n",
        "\n",
        "You can monitor the execution of a pipeline in realtime both from a notebook and using the Azure Portal. \n",
        "\n",
        "Run the following cell and observe the order in which the pipeline steps are executed: **processTrainDataStep** followed by the **trainStep**\n",
        "\n",
        "Wait until both pipeline steps finish running. The cell below should periodically auto-refresh and you can also rerun the cell to force a refresh.\n",
        "\n",
        "Notice in the output of the cell below, at the bottom, there is a link you can click to see the status of the run from within the Azure Portal."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "RunDetails(pipeline_run).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a394aa688b94b6593e9273313171a32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/1378a21d-6b63-48c9-9ad4-65f187f1e210?wsid=/subscriptions/8c12de75-5827-4795-b948-6c427fc4b7ab/resourcegroups/Shelly.Xiao.RG/workspaces/shelly-demo&tid=570057f4-73ef-41c8-bcbb-08db2fc15c2b\", \"run_id\": \"1378a21d-6b63-48c9-9ad4-65f187f1e210\", \"run_properties\": {\"run_id\": \"1378a21d-6b63-48c9-9ad4-65f187f1e210\", \"created_utc\": \"2021-07-14T16:55:47.655217Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": \"2021-07-14T16:58:19.121418Z\", \"status\": \"Completed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://shellydemo4899858397.blob.core.windows.net/azureml/ExperimentRun/dcid.1378a21d-6b63-48c9-9ad4-65f187f1e210/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=nYyAq%2F9N9qV0NA02GMno44LxJ3jKyMKU6Mn7xu4Rzbw%3D&st=2021-07-14T17%3A45%3A57Z&se=2021-07-15T01%3A55%3A57Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://shellydemo4899858397.blob.core.windows.net/azureml/ExperimentRun/dcid.1378a21d-6b63-48c9-9ad4-65f187f1e210/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=5gj%2Bf5almIFFBA1rflrZCYw1sjB%2FvN3ubWkseGv1cMU%3D&st=2021-07-14T17%3A45%3A57Z&se=2021-07-15T01%3A55%3A57Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://shellydemo4899858397.blob.core.windows.net/azureml/ExperimentRun/dcid.1378a21d-6b63-48c9-9ad4-65f187f1e210/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=wQSn1tbtgUqGsclXVOS8S4qDts0ugPhJXIsOhOLKwlo%3D&st=2021-07-14T17%3A45%3A57Z&se=2021-07-15T01%3A55%3A57Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:02:31\", \"run_number\": \"31\", \"run_queued_details\": {\"status\": \"Finished\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"d796b3ae-f106-499c-943e-32bff16f9451\", \"name\": \"train\", \"status\": \"Finished\", \"start_time\": \"2021-07-14T16:57:58.295526Z\", \"created_time\": \"2021-07-14T16:57:46.245124Z\", \"end_time\": \"2021-07-14T16:58:16.191301Z\", \"duration\": \"0:00:29\", \"run_number\": 33, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-07-14T16:57:46.245124Z\", \"is_reused\": \"\"}, {\"run_id\": \"2d14c0f7-3e29-4a99-9cd1-38b768c4f631\", \"name\": \"process_train_data\", \"status\": \"Finished\", \"start_time\": \"2021-07-14T16:56:06.104535Z\", \"created_time\": \"2021-07-14T16:55:50.579837Z\", \"end_time\": \"2021-07-14T16:57:41.474727Z\", \"duration\": \"0:01:50\", \"run_number\": 32, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-07-14T16:55:50.579837Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2021-07-14 16:55:50Z] Submitting 1 runs, first five are: 3c47be96:2d14c0f7-3e29-4a99-9cd1-38b768c4f631\\n[2021-07-14 16:57:45Z] Completing processing run id 2d14c0f7-3e29-4a99-9cd1-38b768c4f631.\\n[2021-07-14 16:57:46Z] Submitting 1 runs, first five are: 609f90f6:d796b3ae-f106-499c-943e-32bff16f9451\\n[2021-07-14 16:58:18Z] Completing processing run id d796b3ae-f106-499c-943e-32bff16f9451.\\n\\nRun is completed.\", \"graph\": {\"datasource_nodes\": {\"2cad3ef2\": {\"node_id\": \"2cad3ef2\", \"name\": \"nyc_taxi_raw_features\"}}, \"module_nodes\": {\"609f90f6\": {\"node_id\": \"609f90f6\", \"name\": \"train\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"d796b3ae-f106-499c-943e-32bff16f9451\"}, \"3c47be96\": {\"node_id\": \"3c47be96\", \"name\": \"process_train_data\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"2d14c0f7-3e29-4a99-9cd1-38b768c4f631\"}}, \"edges\": [{\"source_node_id\": \"3c47be96\", \"source_node_name\": \"process_train_data\", \"source_name\": \"processed_train_data\", \"target_name\": \"processed_train_data\", \"dst_node_id\": \"609f90f6\", \"dst_node_name\": \"train\"}, {\"source_node_id\": \"2cad3ef2\", \"source_node_name\": \"nyc_taxi_raw_features\", \"source_name\": \"data\", \"target_name\": \"nyc_taxi_raw_features\", \"dst_node_id\": \"3c47be96\", \"dst_node_name\": \"process_train_data\"}], \"child_runs\": [{\"run_id\": \"d796b3ae-f106-499c-943e-32bff16f9451\", \"name\": \"train\", \"status\": \"Finished\", \"start_time\": \"2021-07-14T16:57:58.295526Z\", \"created_time\": \"2021-07-14T16:57:46.245124Z\", \"end_time\": \"2021-07-14T16:58:16.191301Z\", \"duration\": \"0:00:29\", \"run_number\": 33, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-07-14T16:57:46.245124Z\", \"is_reused\": \"\"}, {\"run_id\": \"2d14c0f7-3e29-4a99-9cd1-38b768c4f631\", \"name\": \"process_train_data\", \"status\": \"Finished\", \"start_time\": \"2021-07-14T16:56:06.104535Z\", \"created_time\": \"2021-07-14T16:55:50.579837Z\", \"end_time\": \"2021-07-14T16:57:41.474727Z\", \"duration\": \"0:01:50\", \"run_number\": 32, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-07-14T16:55:50.579837Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.31.0\"}, \"loading\": false}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1626281757342
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline Step 3 - Inferencing\n",
        "At this point you have a trained model you can use to begin making predictions. In the following, you will create a new pipeline step that is used for the purpose of inferencing (or scoring)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a DataReference Object to represent the Test data\n",
        "\n",
        "Run the following cell to upload the test data and create the DataReference object (`raw_bulk_test_data`) that references it. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_reference_name = 'raw_bulk_test_data'\n",
        "test_data_path = 'bulk-test-data/raw-test-data.csv'"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626282765815
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataReference object referencing the 'raw-test-data.csv' file\n",
        "raw_bulk_test_data = DataReference(datastore=def_blob_store, \n",
        "                                      data_reference_name=test_data_reference_name, \n",
        "                                      path_on_datastore=test_data_path)\n",
        "print(\"DataReference object created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataReference object created\n"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1626282769308
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Process Test Data Pipeline Step"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the process train data pipeline step, create a new step for processing the test data. Note that it is the same script file `preprocess.py` that is used to process both the train and test data. The key difference is that the process_mode argument is set to *inference*, which the `preprocess.py` script will use to process the test data in a slightly different way than the train data. \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "processed_test_data = PipelineData('processed_test_data', datastore=def_blob_store)\n",
        "print(\"PipelineData object created\")\n",
        "\n",
        "processTestDataStep = PythonScriptStep(\n",
        "    name=\"process_test_data\",\n",
        "    script_name=\"preprocess.py\", \n",
        "    arguments=[\"--process_mode\", 'inference',\n",
        "               \"--input\", raw_bulk_test_data,\n",
        "               \"--output\", processed_test_data],\n",
        "    inputs=[raw_bulk_test_data],\n",
        "    outputs=[processed_test_data],\n",
        "    allow_reuse = False,\n",
        "    compute_target=aml_compute,\n",
        "    runconfig=run_amlcompute,\n",
        "    source_directory=project_folder\n",
        ")\n",
        "print(\"preprocessStep created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PipelineData object created\n",
            "preprocessStep created\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1626282775578
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Inference Pipeline Step to Make Predictions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inference pipeline step takes the `processed_test_data` created in the above step and the `trained_model` created in the train step as two inputs and generates `inference_output` as its output. This is yet another example of how machine learning pipelines can have many steps and these steps could use or reuse datasources and intermediate data.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "inference_output = PipelineData('inference_output', datastore=def_blob_store)\n",
        "print(\"PipelineData object created\")\n",
        "\n",
        "inferenceStep = PythonScriptStep(\n",
        "    name=\"inference\",\n",
        "    script_name=\"inference.py\", \n",
        "    arguments=[\"--input\", processed_test_data,\n",
        "               \"--model\", trained_model,\n",
        "               \"--output\", inference_output],\n",
        "    inputs=[processed_test_data, trained_model],\n",
        "    outputs=[inference_output],\n",
        "    compute_target=aml_compute,\n",
        "    runconfig=run_amlcompute,\n",
        "    source_directory=project_folder\n",
        ")\n",
        "print(\"inferenceStep created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PipelineData object created\n",
            "inferenceStep created\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1626282783103
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and Validate the Pipeline\n",
        "\n",
        "The `inferenceStep` has an implicit data dependency with **ALL** of the previous pipeline steps. So when we create a Pipeline object that lists only `inferenceStep` in the steps array, we are implicitly including `process_test_data`, and the model created by `train`.\n",
        "\n",
        "Run the following cell to create and validate the inferencing pipeline."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "inference_pipeline = Pipeline(workspace=ws, steps=[inferenceStep])\n",
        "print (\"Inference Pipeline is built\")\n",
        "\n",
        "inference_pipeline.validate()\n",
        "print(\"Simple validation complete\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Pipeline is built\n",
            "Step inference is ready to be created [afcac014]\n",
            "Step process_test_data is ready to be created [a94cc92b]\n",
            "Simple validation complete\n"
          ]
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1626282792109
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Publish the Inference Pipeline\n",
        "\n",
        "Publish the inferencing pipeline so it can be executed to score any data that is supplied in a batch fashion.\n",
        "\n",
        "Note that we are not submitting the pipeline to run, instead we are publishing the pipeline. When you publish a pipeline, it can be submitted to run by invoking it via its REST endpoint. \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_name = 'Inference Pipeline'\n",
        "published_pipeline = inference_pipeline.publish(name = pipeline_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created step inference [afcac014][21b795b4-5170-439f-8132-101095357fde], (This step will run and generate new outputs)\n",
            "Created step process_test_data [a94cc92b][e44d884a-7cee-4085-80b3-71bb38446673], (This step will run and generate new outputs)\n",
            "Created step train [f19e7e98][e63e9316-fba3-40e1-abef-a2fbeebabc50], (This step is eligible to reuse a previous run's output)\n",
            "Created step process_train_data [e30c5bfd][873bdeb9-f288-46a7-b952-29d9e42dfbb0], (This step is eligible to reuse a previous run's output)\n",
            "Using data reference raw_bulk_test_data for StepId [c02c64d9][14cb2364-0210-4353-a339-ea6bbdfebbc4], (Consumers of this data are eligible to reuse prior runs.)Using data reference nyc_taxi_raw_features for StepId [ca2bfe1e][1471eda0-0460-46e8-9d99-b6bba7aee8b2], (Consumers of this data are eligible to reuse prior runs.)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1626282802711
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Schedule the Inference Pipeline\n",
        "\n",
        "We want to run the Inference Pipeline when any new data is uploaded at the location referenced by the `raw_bulk_test_data` DataReference object. The next cell creates a Schedule to monitor the datastore for changes, and is responsible for running the `Inference Pipeline` when it detects a new file being uploaded."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core.schedule import Schedule\n",
        "\n",
        "schedule = Schedule.create(workspace=ws, name=pipeline_name + \"_sch\",\n",
        "                           pipeline_id=published_pipeline.id, \n",
        "                           experiment_name=experiment_name,\n",
        "                           datastore=def_blob_store,\n",
        "                           wait_for_provisioning=True,\n",
        "                           description=\"Datastore scheduler for Pipeline: \" + pipeline_name,\n",
        "                           path_on_datastore='bulk-test-data',\n",
        "                           polling_interval=1 # in minutes\n",
        "                           )\n",
        "\n",
        "print(\"Created schedule with id: {}\".format(schedule.id))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Provisioning status: Completed\n",
            "Created schedule with id: 182991c3-f237-465e-a873-6b3dca5996fa\n"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1626282825022
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the status of the Schedule and confirm it's Active\n",
        "print('Schedule status: ', schedule.status)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schedule status:  Active\n"
          ]
        }
      ],
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1626235805663
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Inference Pipeline\n",
        "\n",
        "In the following cell some test data is created and uploaded to the `bulk-test-data` blob store to make bulk predictions. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the raw test data\n",
        "columns = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', 'day_of_month', \n",
        "           'month_num', 'normalizeHolidayName', 'isPaidTimeOff', 'snowDepth', 'precipTime', \n",
        "           'precipDepth', 'temperature']\n",
        "\n",
        "data = [[1, 4, 10, 15, 4, 5, 7, 'None', False, 0, 0.0, 0.0, 80], \n",
        "        [1, 1, 5, 6, 0, 20, 1, 'Martin Luther King, Jr. Day', True, 0, 2.0, 3.0, 35]]\n",
        "\n",
        "data_df = pd.DataFrame(data, columns = columns)\n",
        "\n",
        "os.makedirs(test_data_location, exist_ok=True)\n",
        "data_df.to_csv(os.path.join(test_data_location, 'raw-test-data.csv'), header=True, index=False)\n",
        "\n",
        "from datetime import datetime\n",
        "data_upload_time = datetime.utcnow()\n",
        "print('Data upload time in UTC: ', data_upload_time)\n",
        "\n",
        "# Upload the raw test data to the blob storage\n",
        "def_blob_store.upload(src_dir=test_data_location, \n",
        "                      target_path='bulk-test-data', \n",
        "                      overwrite=True, \n",
        "                      show_progress=True)\n",
        "\n",
        "# Wait for 65 seconds...\n",
        "import time\n",
        "print('Please wait...')\n",
        "time.sleep(65)\n",
        "print('Done!')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data upload time in UTC:  2021-07-14 17:14:03.847666\n",
            "Uploading an estimated of 3 files\n",
            "Uploading aml-pipelines-test-data/.amlignore\n",
            "Uploaded aml-pipelines-test-data/.amlignore, 1 files out of an estimated total of 3\n",
            "Uploading aml-pipelines-test-data/.amlignore.amltmp\n",
            "Uploaded aml-pipelines-test-data/.amlignore.amltmp, 2 files out of an estimated total of 3\n",
            "Uploading aml-pipelines-test-data/raw-test-data.csv\n",
            "Uploaded aml-pipelines-test-data/raw-test-data.csv, 3 files out of an estimated total of 3\n",
            "Uploaded 3 files\n",
            "Please wait...\n",
            "Done!\n"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1626282908947
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wait for Schedule to Trigger\n",
        "\n",
        "The Schedule polling interval is 1 minute. You can also log into Azure Portal and navigate to your `resource group -> workspace -> experiment` to see if the `Inference Pipeline` has started executing.\n",
        "\n",
        "**If the inference_pipeline_run object in the below cell is None, it means that the Schedule has not triggered yet!**\n",
        "\n",
        "**If the Schedule does not trigger in 2 minutes, try rerunning the data upload cell again!**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm that the inference_pipeline_run object is NOT None\n",
        "inference_pipeline_run = schedule.get_last_pipeline_run()\n",
        "print(inference_pipeline_run)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run(Experiment: fleet-pipeline,\n",
            "Id: d4393d5a-4b6a-4c38-bf12-8b3b5e996599,\n",
            "Type: azureml.PipelineRun,\n",
            "Status: Completed)\n"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1626283089694
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*If you upload the test data file more than once, confirm that we have the latest pipeline run object. We will compare the pipeline start time with the time you uploaded the test data file.*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm the start time\n",
        "import dateutil.parser\n",
        "\n",
        "if inference_pipeline_run.get_details()['status'] != 'NotStarted':\n",
        "    pipeline_starttime = dateutil.parser.parse(inference_pipeline_run.get_details()['startTimeUtc'], ignoretz=True)\n",
        "else:\n",
        "    pipeline_starttime = datetime.utcnow()\n",
        "\n",
        "if(pipeline_starttime > data_upload_time):\n",
        "    print('We have the correct inference pipeline run! Proceed to next cell.')\n",
        "else:\n",
        "    print('Rerun the above cell to get the latest inference pipeline run!')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have the correct inference pipeline run! Proceed to next cell.\n"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1626283105223
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitor the Run Details\n",
        "\n",
        "Observe the order in which the pipeline steps are executed based on their implicit data dependencies.\n",
        "\n",
        "Wait until all steps finish running. The cell below should periodically auto-refresh and you can also rerun the cell to force a refresh.\n",
        "\n",
        "**This example demonstrates how AML make it possible to reuse previously completed steps and run only the modified or new steps in your pipeline.**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "RunDetails(inference_pipeline_run).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e82bd6ecb1b4175881433d9d2f964de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/d4393d5a-4b6a-4c38-bf12-8b3b5e996599?wsid=/subscriptions/8c12de75-5827-4795-b948-6c427fc4b7ab/resourcegroups/Shelly.Xiao.RG/workspaces/shelly-demo&tid=570057f4-73ef-41c8-bcbb-08db2fc15c2b\", \"run_id\": \"d4393d5a-4b6a-4c38-bf12-8b3b5e996599\", \"run_properties\": {\"run_id\": \"d4393d5a-4b6a-4c38-bf12-8b3b5e996599\", \"created_utc\": \"2021-07-14T17:14:50.269001Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"Unavailable\", \"runType\": \"Schedule\", \"azureml.parameters\": \"{}\", \"azureml.pipelineid\": \"700054cc-1232-4308-8a11-5ae5c919d63c\"}, \"tags\": {\"azureml.pipelineid\": \"700054cc-1232-4308-8a11-5ae5c919d63c\", \"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": \"2021-07-14T17:17:49.357586Z\", \"status\": \"Completed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://shellydemo4899858397.blob.core.windows.net/azureml/ExperimentRun/dcid.d4393d5a-4b6a-4c38-bf12-8b3b5e996599/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=74WHJXSkE48eYnTmB9%2Bs1f2g2zm95cx9vIHP%2BQ9pUis%3D&st=2021-07-14T18%3A09%3A12Z&se=2021-07-15T02%3A19%3A12Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://shellydemo4899858397.blob.core.windows.net/azureml/ExperimentRun/dcid.d4393d5a-4b6a-4c38-bf12-8b3b5e996599/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=RRE9ELDz55NOD3MxtYLHrTi2yDd908z4dMUrWklA0%2Fs%3D&st=2021-07-14T18%3A09%3A12Z&se=2021-07-15T02%3A19%3A12Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://shellydemo4899858397.blob.core.windows.net/azureml/ExperimentRun/dcid.d4393d5a-4b6a-4c38-bf12-8b3b5e996599/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=rwY6z3r2YWM4nNGB2zEL4v1UECzlE9y7coIkbQrUMwI%3D&st=2021-07-14T18%3A09%3A12Z&se=2021-07-15T02%3A19%3A12Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:02:59\", \"run_number\": \"34\", \"run_queued_details\": {\"status\": \"Finished\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"6e477a36-762e-4242-81c3-d831684c8621\", \"name\": \"inference\", \"status\": \"Finished\", \"start_time\": \"2021-07-14T17:17:19.335685Z\", \"created_time\": \"2021-07-14T17:16:00.470944Z\", \"end_time\": \"2021-07-14T17:17:46.464915Z\", \"duration\": \"0:01:45\", \"run_number\": 47, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-07-14T17:16:00.470944Z\", \"is_reused\": \"\"}, {\"run_id\": \"7505adc8-67a8-4538-b284-7847de618d1f\", \"name\": \"process_test_data\", \"status\": \"Finished\", \"start_time\": \"2021-07-14T17:15:40.973236Z\", \"created_time\": \"2021-07-14T17:14:54.307105Z\", \"end_time\": \"2021-07-14T17:15:57.697991Z\", \"duration\": \"0:01:03\", \"run_number\": 45, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-07-14T17:14:54.307105Z\", \"is_reused\": \"\"}, {\"run_id\": \"709f5cbf-9244-4181-bd93-946ff5bc5c72\", \"name\": \"train\", \"status\": \"Finished\", \"start_time\": \"2021-07-14T17:14:54.002658Z\", \"created_time\": \"2021-07-14T17:14:54.002658Z\", \"end_time\": \"2021-07-14T17:14:54.064303Z\", \"duration\": \"0:00:00\", \"run_number\": 44, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-07-14T17:14:54.002658Z\", \"is_reused\": \"Yes\"}, {\"run_id\": \"22f197aa-27be-4549-b331-e1358344288c\", \"name\": \"process_train_data\", \"status\": \"Finished\", \"start_time\": \"2021-07-14T17:14:53.62398Z\", \"created_time\": \"2021-07-14T17:14:53.62398Z\", \"end_time\": \"2021-07-14T17:14:53.69266Z\", \"duration\": \"0:00:00\", \"run_number\": 41, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-07-14T17:14:53.62398Z\", \"is_reused\": \"Yes\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2021-07-14 17:14:53Z] Completing processing run id 22f197aa-27be-4549-b331-e1358344288c.\\n[2021-07-14 17:14:54Z] Completing processing run id 709f5cbf-9244-4181-bd93-946ff5bc5c72.\\n[2021-07-14 17:14:54Z] Submitting 1 runs, first five are: a94cc92b:7505adc8-67a8-4538-b284-7847de618d1f\\n[2021-07-14 17:16:00Z] Completing processing run id 7505adc8-67a8-4538-b284-7847de618d1f.\\n[2021-07-14 17:16:00Z] Submitting 1 runs, first five are: afcac014:6e477a36-762e-4242-81c3-d831684c8621\\n[2021-07-14 17:17:49Z] Completing processing run id 6e477a36-762e-4242-81c3-d831684c8621.\\n\\nRun is completed.\", \"graph\": {\"datasource_nodes\": {\"c02c64d9\": {\"node_id\": \"c02c64d9\", \"name\": \"raw_bulk_test_data\"}, \"ca2bfe1e\": {\"node_id\": \"ca2bfe1e\", \"name\": \"nyc_taxi_raw_features\"}}, \"module_nodes\": {\"afcac014\": {\"node_id\": \"afcac014\", \"name\": \"inference\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"6e477a36-762e-4242-81c3-d831684c8621\"}, \"a94cc92b\": {\"node_id\": \"a94cc92b\", \"name\": \"process_test_data\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"7505adc8-67a8-4538-b284-7847de618d1f\"}, \"f19e7e98\": {\"node_id\": \"f19e7e98\", \"name\": \"train\", \"status\": \"Finished\", \"_is_reused\": true, \"run_id\": \"709f5cbf-9244-4181-bd93-946ff5bc5c72\"}, \"e30c5bfd\": {\"node_id\": \"e30c5bfd\", \"name\": \"process_train_data\", \"status\": \"Finished\", \"_is_reused\": true, \"run_id\": \"22f197aa-27be-4549-b331-e1358344288c\"}}, \"edges\": [{\"source_node_id\": \"a94cc92b\", \"source_node_name\": \"process_test_data\", \"source_name\": \"processed_test_data\", \"target_name\": \"processed_test_data\", \"dst_node_id\": \"afcac014\", \"dst_node_name\": \"inference\"}, {\"source_node_id\": \"f19e7e98\", \"source_node_name\": \"train\", \"source_name\": \"trained_model\", \"target_name\": \"processed_test_data\", \"dst_node_id\": \"afcac014\", \"dst_node_name\": \"inference\"}, {\"source_node_id\": \"c02c64d9\", \"source_node_name\": \"raw_bulk_test_data\", \"source_name\": \"data\", \"target_name\": \"raw_bulk_test_data\", \"dst_node_id\": \"a94cc92b\", \"dst_node_name\": \"process_test_data\"}, {\"source_node_id\": \"e30c5bfd\", \"source_node_name\": \"process_train_data\", \"source_name\": \"processed_train_data\", \"target_name\": \"processed_train_data\", \"dst_node_id\": \"f19e7e98\", \"dst_node_name\": \"train\"}, {\"source_node_id\": \"ca2bfe1e\", \"source_node_name\": \"nyc_taxi_raw_features\", \"source_name\": \"data\", \"target_name\": \"nyc_taxi_raw_features\", \"dst_node_id\": \"e30c5bfd\", \"dst_node_name\": \"process_train_data\"}], \"child_runs\": [{\"run_id\": \"6e477a36-762e-4242-81c3-d831684c8621\", \"name\": \"inference\", \"status\": \"Finished\", \"start_time\": \"2021-07-14T17:17:19.335685Z\", \"created_time\": \"2021-07-14T17:16:00.470944Z\", \"end_time\": \"2021-07-14T17:17:46.464915Z\", \"duration\": \"0:01:45\", \"run_number\": 47, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-07-14T17:16:00.470944Z\", \"is_reused\": \"\"}, {\"run_id\": \"7505adc8-67a8-4538-b284-7847de618d1f\", \"name\": \"process_test_data\", \"status\": \"Finished\", \"start_time\": \"2021-07-14T17:15:40.973236Z\", \"created_time\": \"2021-07-14T17:14:54.307105Z\", \"end_time\": \"2021-07-14T17:15:57.697991Z\", \"duration\": \"0:01:03\", \"run_number\": 45, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-07-14T17:14:54.307105Z\", \"is_reused\": \"\"}, {\"run_id\": \"709f5cbf-9244-4181-bd93-946ff5bc5c72\", \"name\": \"train\", \"status\": \"Finished\", \"start_time\": \"2021-07-14T17:14:54.002658Z\", \"created_time\": \"2021-07-14T17:14:54.002658Z\", \"end_time\": \"2021-07-14T17:14:54.064303Z\", \"duration\": \"0:00:00\", \"run_number\": 44, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-07-14T17:14:54.002658Z\", \"is_reused\": \"Yes\"}, {\"run_id\": \"22f197aa-27be-4549-b331-e1358344288c\", \"name\": \"process_train_data\", \"status\": \"Finished\", \"start_time\": \"2021-07-14T17:14:53.62398Z\", \"created_time\": \"2021-07-14T17:14:53.62398Z\", \"end_time\": \"2021-07-14T17:14:53.69266Z\", \"duration\": \"0:00:00\", \"run_number\": 41, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-07-14T17:14:53.62398Z\", \"is_reused\": \"Yes\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.31.0\"}, \"loading\": false}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1626283114470
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and Observe the Predictions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Get StepRun for inference step...\")\n",
        "pipeline_run_id = inference_pipeline_run.id\n",
        "step_run_id = inference_pipeline_run.find_step_run('inference')[0].id\n",
        "node_id = inference_pipeline_run.get_graph().node_name_dict['inference'][0].node_id\n",
        "print('Pipeline Run ID: {} Step Run ID: {}, Step Run Node ID: {}'.format(pipeline_run_id, \n",
        "                                                                         step_run_id, \n",
        "                                                                         node_id))\n",
        "step_run = StepRun(inference_pipeline_run.experiment, \n",
        "                   step_run_id, \n",
        "                   pipeline_run_id, \n",
        "                   node_id)\n",
        "print(step_run)\n",
        "\n",
        "print(\"Downloading evaluation results...\")\n",
        "# access the evaluate_output\n",
        "#data = pipeline_run.find_step_run('evaluate')[0].get_output_data('evaluate_output')\n",
        "data = step_run.get_output_data('inference_output')\n",
        "# download the predictions to local path\n",
        "data.download('.', show_progress=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get StepRun for inference step...\n",
            "Pipeline Run ID: d4393d5a-4b6a-4c38-bf12-8b3b5e996599 Step Run ID: 6e477a36-762e-4242-81c3-d831684c8621, Step Run Node ID: afcac014\n",
            "Run(Experiment: fleet-pipeline,\n",
            "Id: 6e477a36-762e-4242-81c3-d831684c8621,\n",
            "Type: azureml.StepRun,\n",
            "Status: Completed)\n",
            "Downloading evaluation results...\n",
            "Downloading azureml/6e477a36-762e-4242-81c3-d831684c8621/inference_output/results.txt\n",
            "Downloaded azureml/6e477a36-762e-4242-81c3-d831684c8621/inference_output/results.txt, 1 files out of an estimated total of 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "1"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1626283150220
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join('./', data.path_on_datastore, 'results.txt')) as f:\n",
        "    results = f.read()\n",
        "print(\"Printing evaluation results...\")\n",
        "print(results)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing evaluation results...\n",
            "40.83369223705172\n",
            "18.003962863409335\n",
            "\n"
          ]
        }
      ],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1626283157667
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleanup Resources\n",
        "\n",
        "If you are done experimenting with this experience, run the following cell to clean up the schedule."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "schedule.disable()"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1626283172503
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Realtime inference"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_location = 'aml-pipelines-models'\n",
        "\n",
        "os.makedirs(model_location, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626283184819
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_run = pipeline_run.find_step_run(\"train\")[0]\n",
        "step_run_output = training_run.get_output(\"trained_model\")\n",
        "\n",
        "port_data_reference = step_run_output.get_port_data_reference()\n",
        "port_data_reference.download(local_path=\"./aml-pipelines-models\")\n",
        "model_path = f'./aml-pipelines-models/azureml/{training_run.id}/trained_model/nyc-taxi-fare.pkl'\n",
        "print(model_path)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./aml-pipelines-models/azureml/d796b3ae-f106-499c-943e-32bff16f9451/trained_model/nyc-taxi-fare.pkl\n"
          ]
        }
      ],
      "execution_count": 30,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626283194848
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# register the model for deployment\n",
        "from azureml.core.model import Model\n",
        "model = Model.register(model_path = model_path,\n",
        "                       model_name = \"predict-nyc-taxi-fare\",\n",
        "                       tags = {'area': \"auto\", 'type': \"regression\"},\n",
        "                       description = \"Contoso Auto model to predict taxi fares in New York City\",\n",
        "                       workspace = ws)\n",
        "\n",
        "print(f'model name: {model.name}, model description: {model.description}, model version: {model.version}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registering model predict-nyc-taxi-fare\n",
            "model name: predict-nyc-taxi-fare, model description: Contoso Auto model to predict taxi fares in New York City, model version: 2\n"
          ]
        }
      ],
      "execution_count": 32,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626283288279
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "testEnv = Environment.from_conda_specification('testEnv', './aml-pipeline-config/aml_pipeline_dependencies.yml')\n",
        "testEnv.register(workspace=ws)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "{\n    \"databricks\": {\n        \"eggLibraries\": [],\n        \"jarLibraries\": [],\n        \"mavenLibraries\": [],\n        \"pypiLibraries\": [],\n        \"rcranLibraries\": []\n    },\n    \"docker\": {\n        \"arguments\": [],\n        \"baseDockerfile\": null,\n        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210531.v1\",\n        \"baseImageRegistry\": {\n            \"address\": null,\n            \"password\": null,\n            \"registryIdentity\": null,\n            \"username\": null\n        },\n        \"enabled\": false,\n        \"platform\": {\n            \"architecture\": \"amd64\",\n            \"os\": \"Linux\"\n        },\n        \"sharedVolumes\": true,\n        \"shmSize\": null\n    },\n    \"environmentVariables\": {\n        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n    },\n    \"inferencingStackVersion\": null,\n    \"name\": \"testEnv\",\n    \"python\": {\n        \"baseCondaEnvironment\": null,\n        \"condaDependencies\": {\n            \"channels\": [\n                \"anaconda\",\n                \"conda-forge\"\n            ],\n            \"dependencies\": [\n                \"python=3.6.2\",\n                {\n                    \"pip\": [\n                        \"azureml-train-automl-runtime==1.13.0\",\n                        \"inference-schema\",\n                        \"azureml-explain-model==1.13.0\",\n                        \"azureml-defaults==1.13.0\"\n                    ]\n                },\n                \"numpy>=1.16.0,<1.19.0\",\n                \"pandas==0.25.1\",\n                \"scikit-learn==0.22.1\",\n                \"py-xgboost<=0.90\",\n                \"fbprophet==0.5\",\n                \"holidays==0.9.11\",\n                \"psutil>=5.2.2,<6.0.0\"\n            ],\n            \"name\": \"azureml_8cafa4fedef7ba32d2688b5293ee32e6\"\n        },\n        \"condaDependenciesFile\": null,\n        \"interpreterPath\": \"python\",\n        \"userManagedDependencies\": false\n    },\n    \"r\": null,\n    \"spark\": {\n        \"packages\": [],\n        \"precachePackages\": true,\n        \"repositories\": []\n    },\n    \"version\": \"1\"\n}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 37,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626283428903
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AciWebservice\n",
        "\n",
        "inference_config = InferenceConfig(entry_script='./aml-pipelines-scripts/scoring_service.py', environment=testEnv)\n",
        "aci_config = AciWebservice.deploy_configuration(\n",
        "    cpu_cores = 1, \n",
        "    memory_gb = 1, \n",
        "    tags = {'name': 'aci-cluster'}, \n",
        "    description = 'Scoring web service.')\n",
        "\n",
        "from azureml.core import Webservice\n",
        "\n",
        "service_name = 'predict-nyc-taxi-fare'\n",
        "\n",
        "webservice = Model.deploy(workspace=ws,\n",
        "                       name=service_name,\n",
        "                       models=[model],\n",
        "                       inference_config=inference_config,\n",
        "                       deployment_config=aci_config, \n",
        "                       overwrite=True)\n",
        "webservice.wait_for_deployment(show_output=True)\n",
        "print(webservice.state)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
            "Running\n",
            "2021-07-14 18:05:40+00:00 Creating Container Registry if not exists.\n",
            "2021-07-14 18:05:40+00:00 Registering the environment.\n",
            "2021-07-14 18:05:41+00:00 Use the existing image.\n",
            "2021-07-14 18:05:42+00:00 Generating deployment configuration.\n",
            "2021-07-14 18:05:42+00:00 Submitting deployment to compute.\n",
            "2021-07-14 18:05:45+00:00 Checking the status of deployment predict-nyc-taxi-fare..\n",
            "2021-07-14 18:10:54+00:00 Checking the status of inference endpoint predict-nyc-taxi-fare.\n",
            "Succeeded\n",
            "ACI service creation operation finished, operation \"Succeeded\"\n",
            "Healthy\n"
          ]
        }
      ],
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626286255598
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the deployed web service"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the deployed web service ready, we can test calling the service to see the scored results. There are three ways to approach this: \n",
        "1. You could use the Webservice object that you acquired in the previous cell to call the service directly. \n",
        "2. You could use the Webservice class to get a reference to a deployed web service by name. \n",
        "3. You could use any client capable of making a REST call.\n",
        "\n",
        "In this notebook, we will take the first approach. Run the following cells to retrieve the web service by name and then to invoke it using test data.\n",
        "\n",
        "The output of this cell will be an array of numbers, where each number represents the expected taxi fare."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# load some test vehicle data that the model has not seen\n",
        "test_data = pd.read_csv(os.path.join(test_data_location, 'raw-test-data.csv'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.02 ms, sys: 3.67 ms, total: 6.69 ms\n",
            "Wall time: 44.1 ms\n"
          ]
        }
      ],
      "execution_count": 44,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_sin_cosine(value, max_value, is_zero_base = False):\n",
        "    if not is_zero_base:\n",
        "        value = value - 1\n",
        "    sine =  np.sin(value * (2.*np.pi/max_value))\n",
        "    cosine = np.cos(value * (2.*np.pi/max_value))\n",
        "    return (sine, cosine)\n",
        "\n",
        "test_data[['hour_sine', 'hour_cosine']] = test_data['hour_of_day'].apply(lambda x: \n",
        "                                                               pd.Series(get_sin_cosine(x, 24, True)))\n",
        "\n",
        "# Day of week is a cyclical feature ranging from 0 to 6\n",
        "test_data[['day_of_week_sine', 'day_of_week_cosine']] = test_data['day_of_week'].apply(lambda x: \n",
        "                                                                             pd.Series(get_sin_cosine(x, 7, True)))\n",
        "columns = ['vendorID', 'passengerCount', 'tripDistance', 'hour_sine', 'hour_cosine', \n",
        "           'day_of_week_sine', 'day_of_week_cosine', 'day_of_month', 'month_num', \n",
        "           'normalizeHolidayName', 'isPaidTimeOff', 'snowDepth', 'precipTime', \n",
        "           'precipDepth', 'temperature']\n",
        "\n",
        "test_data = test_data[columns]\n"
      ],
      "outputs": [],
      "execution_count": 45,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626287780970
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the data and select five vehicles\n",
        "test_data_json = test_data.to_json(orient=\"split\")\n",
        "prediction = webservice.run(input_data = test_data_json)\n",
        "print(prediction)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"result\": [40.83369223705172, 18.003962863409335]}\n"
          ]
        }
      ],
      "execution_count": 46,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626287784543
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}